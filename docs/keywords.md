# Global Keyword Index

This index contains all keywords extracted from the repository, sorted alphabetically.

## 2020

- **[cohorts/2024/workshops/rising-wave.md](./cohorts/2024/workshops/rising-wave.md_docs.md)**: For example if the latest pickup time is 2020-01-01 17:00:00,

## API

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: Most data is stored behind an API

## APIs

- **[cohorts/2024/workshops/dlt.md](./cohorts/2024/workshops/dlt.md_docs.md)**: * â€‹Extracting data from APIs, or files.
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: âœ… **Extracting** data from various sources (APIs, databases, files).

## AWS

- **[cohorts/2022/week_2_data_ingestion/README.md](./cohorts/2022/week_2_data_ingestion/README.md_docs.md)**: ### Transfer service (AWS -> GCP)

## Aaron

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Aaron](https://github.com/ABZ-Aaron/DataEngineerZoomCamp/blob/master/week_1_basics_n_
- **[cohorts/2022/week_2_data_ingestion/README.md](./cohorts/2022/week_2_data_ingestion/README.md_docs.md)**: * [Notes from Aaron Wright](https://github.com/ABZ-Aaron/DataEngineer

## Abd

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Abd](https://itnadigital.notion.site/Week-1-Introduct
- **[cohorts/2022/week_2_data_ingestion/README.md](./cohorts/2022/week_2_data_ingestion/README.md_docs.md)**: * [Notes from Abd](https://itnadigital.notion.site/Week-2-Data-Inge

## Alvaro

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Alvaro Navas](https://github.com/ziritrion/dataeng-zoomc
- **[cohorts/2022/week_2_data_ingestion/README.md](./cohorts/2022/week_2_data_ingestion/README.md_docs.md)**: * [Notes from Alvaro Navas](https://github.com/ziritrion/dataeng-zoomc

## Apurva

- **[cohorts/2022/week_2_data_ingestion/README.md](./cohorts/2022/week_2_data_ingestion/README.md_docs.md)**: * [Notes from Apurva Hegde](https://github.com/apuhegde/Airflow-LocalE

## AvroDeserializer

- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from confluent_kafka.schema_registry.avro import AvroDeserializer

## AvroProducer

- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: public class AvroProducer {

## AvroSerializer

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from confluent_kafka.schema_registry.avro import AvroSerializer

## Azure

- **[projects/datasets.md](./projects/datasets.md_docs.md)**: * [Datasets from Azure](https://docs.microsoft.com/en-us/azure/azure-sql

## BOOTSTRAP_SERVERS

- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, INPUT_DATA_PATH, KAFKA_TOPIC
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, KAFKA_TOPIC
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, SCHEMA_REGISTRY_URL, \
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, INPUT_DATA_PATH, PRODUCE_TOPIC_RIDES_CSV
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, CONSUME_TOPIC_RIDES_CSV
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, INPUT_DATA_PATH, PRODUCE_TOPIC_RIDES_CSV
- **[06-streaming/python/streams-example/redpanda/consumer.py](./06-streaming/python/streams-example/redpanda/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, CONSUME_TOPIC_RIDES_CSV
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, INPUT_DATA_PATH, KAFKA_TOPIC
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, KAFKA_TOPIC

## Balaji

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Balaji](https://github.com/Balajirvp/DE-Zoomcamp/blob/ma
- **[04-analytics-engineering/README.md](./04-analytics-engineering/README.md_docs.md)**: * [Notes from Balaji](https://github.com/Balajirvp/DE-Zoomcamp/blob/ma
- **[cohorts/2023/week_2_workflow_orchestration/README.md](./cohorts/2023/week_2_workflow_orchestration/README.md_docs.md)**: * [Notes from Balaji](https://github.com/Balajirvp/DE-Zoomcamp/blob/ma

## BashOperator

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from airflow.operators.bash import BashOperator
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from airflow.operators.bash import BashOperator
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py_docs.md)**: from airflow.operators.bash import BashOperator
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: from airflow.operators.bash import BashOperator

## BigQuery

- **[projects/datasets.md](./projects/datasets.md_docs.md)**: * [Datasets from BigQuery](https://cloud.google.com/bigquery/public-data/)
- **[03-data-warehouse/README.md](./03-data-warehouse/README.md_docs.md)**: # Data Warehouse and BigQuery

## BigQueryCreateExternalTableOperator

- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: .providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
- **[cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py](./cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py_docs.md)**: .providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator, BigQueryInser...

## Builder

- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java_docs.md)**: ic static schemaregistry.RideRecordNoneCompatible.Builder newBuilder() {
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java_docs.md)**: public static schemaregistry.RideRecordCompatible.Builder newBuilder() {
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java_docs.md)**: public static schemaregistry.RideRecord.Builder newBuilder() {

## BytesIO

- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "from io import BytesIO"

## CONFLUENT_CLOUD_CONFIG

- **[cohorts/2023/week_6_stream_processing/streaming_confluent.py](./cohorts/2023/week_6_stream_processing/streaming_confluent.py_docs.md)**: from settings import CONFLUENT_CLOUD_CONFIG, GREEN_TAXI_TOPIC, FHV_TAXI_TOPIC, RIDES_TOPIC, A
- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from settings import CONFLUENT_CLOUD_CONFIG, \

## Candace

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Candace Williams](https://teacherc.github.io/data-enginee

## Colab

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "colab": {

## Consumer

- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from confluent_kafka import Consumer

## CustomSerdes

- **[06-streaming/java/kafka_examples/src/main/java/org/example/customserdes/CustomSerdes.java](./06-streaming/java/kafka_examples/src/main/java/org/example/customserdes/CustomSerdes.java_docs.md)**: package org.example.customserdes;

## DAG

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py](./cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py_docs.md)**: from airflow import DAG

## Daniel

- **[03-data-warehouse/README.md](./03-data-warehouse/README.md_docs.md)**: * [2025 Notes from Daniel Lachner](https://drive.google.com/file/d/105zjtLF

## DataGeneratorHelper

- **[06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java](./06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java_docs.md)**: public class DataGeneratorHelper {

## Decimal

- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: from decimal import Decimal
- **[06-streaming/python/redpanda_example/ride.py](./06-streaming/python/redpanda_example/ride.py_docs.md)**: from decimal import Decimal

## Desktop

- **[after-sign-up.md](./after-sign-up.md_docs.md)**: 3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ) (it works from Desktop only)
- **[cohorts/2022/README.md](./cohorts/2022/README.md_docs.md)**: 3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ) (it works from Desktop only)

## Development

- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: - [Moving from Development to Production](https://go.kestra.io/de-zoomcamp/d

## Dict

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from typing import Dict
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from typing import Dict, List
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from typing import Dict
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from typing import Dict, List
- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: from typing import Dict
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: from typing import Dict
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: from typing import Dict, List
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: from typing import Dict
- **[06-streaming/python/streams-example/redpanda/consumer.py](./06-streaming/python/streams-example/redpanda/consumer.py_docs.md)**: from typing import Dict, List
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: from typing import Dict, List

## Duration

- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: from pyflink.common.time import Duration

## EnvironmentSettings

- **[06-streaming/pyflink/src/job/taxi_job.py](./06-streaming/pyflink/src/job/taxi_job.py_docs.md)**: from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironm
- **[06-streaming/pyflink/src/job/start_job.py](./06-streaming/pyflink/src/job/start_job.py_docs.md)**: from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironm
- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironm

## Erik

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Erik](https://twitter.com/ehub96/status/16213512662817

## Event

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: The collected data flows into an event queue, where itâ€™s organized and prepared for the

## Exercise

- **[cohorts/2024/workshops/dlt_resources/homework_starter.ipynb](./cohorts/2024/workshops/dlt_resources/homework_starter.ipynb_docs.md)**: "Re-use the generators from Exercise 2.\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "Re-use the generators from Exercise 2.\n",

## Faisal

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Faisal](https://github.com/FaisalMohd/data-engineering-z

## GCS

- **[cohorts/2024/02-workflow-orchestration/README.md](./cohorts/2024/02-workflow-orchestration/README.md_docs.md)**: * [2.2.4 - ðŸ¤“ ETL: API to GCS](#224----etl-api-to-gcs)
- **[cohorts/2023/week_2_workflow_orchestration/homework.md](./cohorts/2023/week_2_workflow_orchestration/homework.md_docs.md)**: etl_web_to_gcs.py` flow that loads taxi data into GCS as a guide, create a flow that loads the green...
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: dlt_method = input("Choose loading method: 1 for GCS -> Bigquery, 2 for Direct Web -> Bigquery: ")

## GCSToGCSOperator

- **[cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py](./cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py_docs.md)**: roviders.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator

## Git

- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: insights. The flow will sync the dbt models from Git to Kestra and run the `dbt build` command to bu...

## GitHub

- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: n using [Kestra](https://go.kestra.io/de-zoomcamp/github).

## Google

- **[projects/datasets.md](./projects/datasets.md_docs.md)**: * [Datasets from BigQuery](https://cloud.google.com/bigquery/public-data/)

## Hammad

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Hammad Tariq](https://github.com/hamad-tariq/HammadTariq

## Horeb

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Horeb SEIDOU](https://spotted-hardhat-eea.notion.site/W
- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: * [Notes from Horeb Seidou](https://spotted-hardhat-eea.notion.site/W
- **[03-data-warehouse/README.md](./03-data-warehouse/README.md_docs.md)**: * [Notes from Horeb SEIDOU](https://spotted-hardhat-eea.notion.site/W

## Isaac

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Blog post from Isaac Kargar](https://kargarisaac.github.io/blog/data%2

## JSON

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: xample, we created a simple http api that returns json \"page by page\", 1000 records per page.\n",

## January

- **[cohorts/2022/week_1_basics_n_setup/homework.md](./cohorts/2022/week_1_basics_n_setup/homework.md_docs.md)**: We'll use the yellow taxi trips from January 2021:
- **[cohorts/2023/week_1_docker_sql/homework.md](./cohorts/2023/week_1_docker_sql/homework.md_docs.md)**: We'll use the green taxi trips from January 2019:

## Jonah

- **[cohorts/2024/02-workflow-orchestration/README.md](./cohorts/2024/02-workflow-orchestration/README.md_docs.md)**: * [Notes from Jonah Oliver](https://www.jonahboliver.com/blog/de-zc-w

## JsonConsumer

- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: class JsonConsumer:
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: class JsonConsumer:
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java_docs.md)**: public class JsonConsumer {

## JsonKStream

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: public class JsonKStream {

## JsonKStreamJoins

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: public class JsonKStreamJoins {

## JsonKStreamJoinsTest

- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java_docs.md)**: class JsonKStreamJoinsTest {

## JsonKStreamTest

- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java_docs.md)**: class JsonKStreamTest {

## JsonKStreamWindow

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: public class JsonKStreamWindow {

## JsonProducer

- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: class JsonProducer(KafkaProducer):
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: class JsonProducer(KafkaProducer):
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: public class JsonProducer {

## JsonProducerPickupLocation

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java_docs.md)**: public class JsonProducerPickupLocation {

## Kafka

- **[cohorts/2023/week_6_stream_processing/homework.md](./cohorts/2023/week_6_stream_processing/homework.md_docs.md)**: session focus on theoretical questions related to Kafka
- **[06-streaming/pyflink/src/job/taxi_job.py](./06-streaming/pyflink/src/job/taxi_job.py_docs.md)**: 'connector' = 'kafka',
- **[06-streaming/pyflink/src/job/start_job.py](./06-streaming/pyflink/src/job/start_job.py_docs.md)**: 'connector' = 'kafka',
- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: 'connector' = 'kafka',
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from kafka import KafkaConsumer
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: from kafka import KafkaConsumer
- **[06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb](./06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb_docs.md)**: T_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_...
- **[06-streaming/python/streams-example/redpanda/consumer.py](./06-streaming/python/streams-example/redpanda/consumer.py_docs.md)**: from kafka import KafkaConsumer
- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: T_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_...
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: from kafka import KafkaConsumer

## KafkaConsumer

- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from kafka import KafkaConsumer
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: from kafka import KafkaConsumer
- **[06-streaming/python/streams-example/redpanda/consumer.py](./06-streaming/python/streams-example/redpanda/consumer.py_docs.md)**: from kafka import KafkaConsumer
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: from kafka import KafkaConsumer

## KafkaProducer

- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: from kafka import KafkaProducer
- **[cohorts/2025/06-streaming/homework.md](./cohorts/2025/06-streaming/homework.md_docs.md)**: from kafka import KafkaProducer
- **[cohorts/2025/06-streaming/homework/homework.ipynb](./cohorts/2025/06-streaming/homework/homework.ipynb_docs.md)**: "from kafka import KafkaProducer\n",
- **[06-streaming/pyflink/homework.md](./06-streaming/pyflink/homework.md_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/pyflink/src/producers/producer.py](./06-streaming/pyflink/src/producers/producer.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/pyflink/src/producers/load_taxi_data.py](./06-streaming/pyflink/src/producers/load_taxi_data.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/python/streams-example/faust/producer_taxi_json.py](./06-streaming/python/streams-example/faust/producer_taxi_json.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: from kafka import KafkaProducer
- *(... and 1 more occurrences)*

## KafkaTimeoutError

- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from kafka.errors import KafkaTimeoutError
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: from kafka.errors import KafkaTimeoutError

## Kirill

- **[cohorts/2024/02-workflow-orchestration/README.md](./cohorts/2024/02-workflow-orchestration/README.md_docs.md)**: * [Notes from Kirill](https://github.com/kirill505/data-engineering-zo

## Linda

- **[cohorts/2024/02-workflow-orchestration/README.md](./cohorts/2024/02-workflow-orchestration/README.md_docs.md)**: * [Notes from Linda](https://github.com/inner-outer-space/de-zoomcamp

## List

- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from typing import List, Dict
- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: from typing import List, Dict
- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: from typing import List, Dict
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: from typing import List, Dict
- **[06-streaming/python/redpanda_example/ride.py](./06-streaming/python/redpanda_example/ride.py_docs.md)**: from typing import List, Dict

## Livia

- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: * [Notes from Livia](https://docs.google.com/document/d/1Y_QMonvEtFPb

## Mage

- **[03-data-warehouse/README.md](./03-data-warehouse/README.md_docs.md)**: * [2024 - steps to send data from Mage to GCS + creating external table](https://drive.g

## Manuel

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Manuel Guerra (Windows+WSL2 Environment)](https://github
- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: * [Notes from Manuel Guerra)](https://github.com/ManuelGuerra1987/data

## Marcos

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Marcos Torregrosa](https://www.n4gash.com/2023/data-engi

## Mercy

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Mercy Markus: Linux/Fedora Tweaks and Tips](https://mer
- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: * [Notes from Mercy Markus: Linux/Fedora Tweaks and Tips](https://mer

## Module

- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: ## Module 6 Homework
- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: Welcome to Module 2 of the Data Engineering Zoomcamp! This week, we

## NYC

- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: ETL pipelines for Yellow and Green Taxi data from NYCâ€™s Taxi and Limousine Commission (TLC). You wil...

## NotFound

- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: from google.api_core.exceptions import NotFound, Forbidden

## October

- **[cohorts/2025/01-docker-terraform/homework.md](./cohorts/2025/01-docker-terraform/homework.md_docs.md)**: We'll use the green taxi trips from October 2019:

## PageNumberPaginator

- **[cohorts/2025/workshops/dlt/dlt_homework.md](./cohorts/2025/workshops/dlt/dlt_homework.md_docs.md)**: dlt.sources.helpers.rest_client.paginators import PageNumberPaginator
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: dlt.sources.helpers.rest_client.paginators import PageNumberPaginator

## Path

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: from pathlib import Path

## PickupLocation

- **[06-streaming/java/kafka_examples/src/main/java/org/example/data/PickupLocation.java](./06-streaming/java/kafka_examples/src/main/java/org/example/data/PickupLocation.java_docs.md)**: public class PickupLocation {

## Producer

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from confluent_kafka import Producer
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from confluent_kafka import Producer

## PythonOperator

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from airflow.operators.python import PythonOperator
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from airflow.operators.python import PythonOperator
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py_docs.md)**: from airflow.operators.python import PythonOperator
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: from airflow.operators.python import PythonOperator

## RESTClient

- **[cohorts/2025/workshops/dlt/dlt_homework.md](./cohorts/2025/workshops/dlt/dlt_homework.md_docs.md)**: from dlt.sources.helpers.rest_client import RESTClient
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: lly writing pagination logic, letâ€™s use **dltâ€™s [`RESTClient` helper](https://dlthub.com/docs/genera...

## RIDE_KEY_SCHEMA_PATH

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from settings import RIDE_KEY_SCHEMA_PATH, RIDE_VALUE_SCHEMA_PATH, \

## RIDE_SCHEMA

- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: from settings import RIDE_SCHEMA, CONSUME_TOPIC_RIDES_CSV, TOPIC_WINDOWED_VENDOR_I
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: from settings import RIDE_SCHEMA, CONSUME_TOPIC_RIDES_CSV, TOPIC_WINDOWED_VENDOR_I

## RIDE_STREAMS

- **[06-streaming/ksqldb/commands.md](./06-streaming/ksqldb/commands.md_docs.md)**: CREATE STREAM ride_streams (

## RepoBookGenerator

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: class RepoBookGenerator:

## Ride

- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from ride import Ride
- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: class Ride:
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from ride import Ride
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: from ride import Ride
- **[06-streaming/python/redpanda_example/ride.py](./06-streaming/python/redpanda_example/ride.py_docs.md)**: class Ride:
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: from ride import Ride
- **[06-streaming/java/kafka_examples/src/main/java/org/example/data/Ride.java](./06-streaming/java/kafka_examples/src/main/java/org/example/data/Ride.java_docs.md)**: public class Ride {

## RideAvroConsumer

- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: class RideAvroConsumer:

## RideAvroProducer

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: class RideAvroProducer:

## RideCSVConsumer

- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: class RideCSVConsumer:
- **[06-streaming/python/streams-example/redpanda/consumer.py](./06-streaming/python/streams-example/redpanda/consumer.py_docs.md)**: class RideCSVConsumer:

## RideCSVProducer

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: class RideCSVProducer:
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: class RideCSVProducer:
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: class RideCSVProducer:

## RideRecord

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from ride_record import RideRecord, ride_record_to_dict
- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: class RideRecord:
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java_docs.md)**: public class RideRecord extends org.apache.avro.specific.SpecificRecordBa

## RideRecordCompatible

- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java_docs.md)**: public class RideRecordCompatible extends org.apache.avro.specific.SpecificRecordBa

## RideRecordKey

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from ride_record_key import RideRecordKey, ride_record_key_to_dict
- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: class RideRecordKey:

## RideRecordNoneCompatible

- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java_docs.md)**: public class RideRecordNoneCompatible extends org.apache.avro.specific.SpecificRecordBa

## Sara

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: * [Data ingestion with DLT to Bigquery from Sara Sabater](https://github.com/saraisab/Data_Enginee

## SchemaRegistryClient

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from confluent_kafka.schema_registry import SchemaRegistryClient
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from confluent_kafka.schema_registry import SchemaRegistryClient

## Secrets

- **[06-streaming/java/kafka_examples/src/main/java/org/example/Secrets.java](./06-streaming/java/kafka_examples/src/main/java/org/example/Secrets.java_docs.md)**: public class Secrets {

## September

- **[cohorts/2024/01-docker-terraform/homework.md](./cohorts/2024/01-docker-terraform/homework.md_docs.md)**: We'll use the green taxi trips from September 2019:

## SerializationContext

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from confluent_kafka.serialization import SerializationContext, MessageField
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from confluent_kafka.serialization import SerializationContext, MessageField

## SparkConf

- **[05-batch/code/09_spark_gcs.ipynb](./05-batch/code/09_spark_gcs.ipynb_docs.md)**: "from pyspark.conf import SparkConf\n",

## SparkContext

- **[05-batch/code/09_spark_gcs.ipynb](./05-batch/code/09_spark_gcs.ipynb_docs.md)**: "from pyspark.context import SparkContext"

## SparkSession

- **[05-batch/code/06_spark_sql.py](./05-batch/code/06_spark_sql.py_docs.md)**: from pyspark.sql import SparkSession
- **[05-batch/code/09_spark_gcs.ipynb](./05-batch/code/09_spark_gcs.ipynb_docs.md)**: "from pyspark.sql import SparkSession\n",
- **[05-batch/code/03_test.ipynb](./05-batch/code/03_test.ipynb_docs.md)**: "from pyspark.sql import SparkSession"
- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "from pyspark.sql import SparkSession\n",
- **[05-batch/code/04_pyspark.ipynb](./05-batch/code/04_pyspark.ipynb_docs.md)**: "from pyspark.sql import SparkSession"
- **[05-batch/code/06_spark_sql.ipynb](./05-batch/code/06_spark_sql.ipynb_docs.md)**: "from pyspark.sql import SparkSession\n",
- **[05-batch/code/homework.ipynb](./05-batch/code/homework.ipynb_docs.md)**: "from pyspark.sql import SparkSession\n",
- **[05-batch/code/07_groupby_join.ipynb](./05-batch/code/07_groupby_join.ipynb_docs.md)**: "from pyspark.sql import SparkSession\n",
- **[05-batch/code/05_taxi_schema.ipynb](./05-batch/code/05_taxi_schema.ipynb_docs.md)**: "from pyspark.sql import SparkSession"
- **[05-batch/code/06_spark_sql_big_query.py](./05-batch/code/06_spark_sql_big_query.py_docs.md)**: from pyspark.sql import SparkSession
- *(... and 9 more occurrences)*

## StreamExecutionEnvironment

- **[06-streaming/pyflink/src/job/taxi_job.py](./06-streaming/pyflink/src/job/taxi_job.py_docs.md)**: from pyflink.datastream import StreamExecutionEnvironment
- **[06-streaming/pyflink/src/job/start_job.py](./06-streaming/pyflink/src/job/start_job.py_docs.md)**: from pyflink.datastream import StreamExecutionEnvironment
- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: from pyflink.datastream import StreamExecutionEnvironment

## TaxiRide

- **[06-streaming/python/streams-example/faust/taxi_rides.py](./06-streaming/python/streams-example/faust/taxi_rides.py_docs.md)**: class TaxiRide(faust.Record, validation=True):
- **[06-streaming/python/streams-example/faust/stream.py](./06-streaming/python/streams-example/faust/stream.py_docs.md)**: from taxi_rides import TaxiRide
- **[06-streaming/python/streams-example/faust/windowing.py](./06-streaming/python/streams-example/faust/windowing.py_docs.md)**: from taxi_rides import TaxiRide
- **[06-streaming/python/streams-example/faust/branch_price.py](./06-streaming/python/streams-example/faust/branch_price.py_docs.md)**: from taxi_rides import TaxiRide
- **[06-streaming/python/streams-example/faust/stream_count_vendor_trips.py](./06-streaming/python/streams-example/faust/stream_count_vendor_trips.py_docs.md)**: from taxi_rides import TaxiRide

## ThreadPoolExecutor

- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: from concurrent.futures import ThreadPoolExecutor

## Tinker0425

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [2025 Gitbook Notes from Tinker0425](https://data-engineering-zoomcamp-2025-t.gitbook
- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: * [2025 Gitbook Notes from Tinker0425](https://data-engineering-zoomcamp-2025-t.gitbook

## Topics

- **[06-streaming/java/kafka_examples/src/main/java/org/example/Topics.java](./06-streaming/java/kafka_examples/src/main/java/org/example/Topics.java_docs.md)**: public class Topics {

## VendorInfo

- **[06-streaming/java/kafka_examples/src/main/java/org/example/data/VendorInfo.java](./06-streaming/java/kafka_examples/src/main/java/org/example/data/VendorInfo.java_docs.md)**: public class VendorInfo {

## Victor

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Victor Padilha](https://github.com/padilha/de-zoomcamp/t
- **[cohorts/2023/week_2_workflow_orchestration/README.md](./cohorts/2023/week_2_workflow_orchestration/README.md_docs.md)**: * [Notes from Victor Padilha](https://github.com/padilha/de-zoomcamp/t

## Vincenzo

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talk
- **[04-analytics-engineering/README.md](./04-analytics-engineering/README.md_docs.md)**: * [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talk
- **[cohorts/2022/week_2_data_ingestion/README.md](./cohorts/2022/week_2_data_ingestion/README.md_docs.md)**: * [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talk
- **[03-data-warehouse/README.md](./03-data-warehouse/README.md_docs.md)**: * [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talk

## WatermarkStrategy

- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: from pyflink.common.watermark_strategy import WatermarkStrategy

## Webhooks

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: - from Webhooks to event queues;

## Xia

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Xia He-Bleinagel](https://xiahe-bleinagel.com/2023/01
- **[cohorts/2023/week_2_workflow_orchestration/README.md](./cohorts/2023/week_2_workflow_orchestration/README.md_docs.md)**: * [Notes from Xia He-Bleinagel](https://xiahe-bleinagel.com/2023/02
- **[03-data-warehouse/README.md](./03-data-warehouse/README.md_docs.md)**: * [Notes from Xia He-Bleinagel](https://xiahe-bleinagel.com/2023/02

## Zharko

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from Zharko Cekovski](https://www.zharconsulting.com/contents
- **[cohorts/2024/02-workflow-orchestration/README.md](./cohorts/2024/02-workflow-orchestration/README.md_docs.md)**: * [Notes from Zharko](https://www.zharconsulting.com/contents/data/dat

## __init__

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def __init__(self):
- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: def __init__(self, probs: Dict, ride_type: str):
- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: def __init__(self, props: Dict):
- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: def __init__(self, arr: List[str]):
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: def __init__(self, props: Dict):
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: def __init__(self, props: Dict):
- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: def __init__(self, arr: List[str]):
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: def __init__(self, props: Dict):
- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: def __init__(self, vendor_id):
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: def __init__(self, props: Dict):
- *(... and 6 more occurrences)*

## __repr__

- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: def __repr__(self):
- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: def __repr__(self):
- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: def __repr__(self):
- **[06-streaming/python/redpanda_example/ride.py](./06-streaming/python/redpanda_example/ride.py_docs.md)**: def __repr__(self):

## _kw

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: """Generate _kw.md for a single file"""

## above

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: xactly the same data as we did in the API example above, but now we get it from the underlying file ...
- **[06-streaming/python/redpanda_example/README.md](./06-streaming/python/redpanda_example/README.md_docs.md)**: ditional information about a consumer group, from above listed result

## adamiaonr

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from adamiaonr](https://github.com/adamiaonr/data-engineering-zoomcamp/blob/main/week_1_bas...

## agriculture

- **[projects/datasets.md](./projects/datasets.md_docs.md)**: e-collection-data-repositories-part-1.html) (from agriculture and finance to government)

## airflow

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: from airflow import DAG
- **[cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py](./cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py_docs.md)**: from airflow import DAG

## always

- **[04-analytics-engineering/SQL_refresher.md](./04-analytics-engineering/SQL_refresher.md_docs.md)**: A window function always has two components. This second part here defines

## any

- **[cohorts/2024/04-analytics-engineering/homework.md](./cohorts/2024/04-analytics-engineering/homework.md_docs.md)**: - The code from any development branch that has been opened based on

## apis

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: In this approach to grabbing data from apis, we have pros and cons:

## apply_model_in_batch

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "def apply_model_in_batch(rows):\n",

## argparse

- **[05-batch/code/06_spark_sql.py](./05-batch/code/06_spark_sql.py_docs.md)**: import argparse
- **[05-batch/code/06_spark_sql_big_query.py](./05-batch/code/06_spark_sql_big_query.py_docs.md)**: import argparse
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: import argparse, os, sys
- **[01-docker-terraform/2_docker_sql/ingest_data.py](./01-docker-terraform/2_docker_sql/ingest_data.py_docs.md)**: import argparse
- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: import argparse
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: import argparse
- **[06-streaming/python/streams-example/redpanda/consumer.py](./06-streaming/python/streams-example/redpanda/consumer.py_docs.md)**: import argparse

## arrivalTime

- **[06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java](./06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java_docs.md)**: var arrivalTime = LocalDateTime.now().format(DateTimeFormatter.of

## auth

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "from google.colab import auth\n",

## backed

- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java_docs.md)**: rn a BinaryMessageDecoder instance for this class backed by the given SchemaStore
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java_docs.md)**: rn a BinaryMessageDecoder instance for this class backed by the given SchemaStore
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java_docs.md)**: rn a BinaryMessageDecoder instance for this class backed by the given SchemaStore

## bigquery

- **[04-analytics-engineering/dbt_cloud_setup.md](./04-analytics-engineering/dbt_cloud_setup.md_docs.md)**: - [How to setup dbt cloud with bigquery](#how-to-setup-dbt-cloud-with-bigquery)
- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "# We now use duckdb, but you can switch to Bigquery later\n",

## bootstrap

- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: " .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n"

## breaking

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: ider on extraction, to prevent the pipelines from breaking, and to keep them running smoothly.
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: ider on extraction, to prevent the pipelines from breaking, and to keep them running smoothly:

## build_comprehensive_book

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def build_comprehensive_book(self):

## build_global_keywords

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def build_global_keywords(self):

## build_root_index

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def build_root_index(self):

## business

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: - **RESTful APIs**: Provide records of data from business applications.

## buttonEl

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " const buttonEl =\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " const buttonEl =\n",

## calculate_revenue

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "def calculate_revenue(left_value, right_value):\n",

## central

- **[06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb](./06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb_docs.md)**: g.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: g.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",

## charts

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " title=\"Suggest charts\"\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " title=\"Suggest charts\"\n",

## clause

- **[cohorts/2024/03-data-warehouse/homework.md](./cohorts/2024/03-data-warehouse/homework.md_docs.md)**: terialized table you created earlier in your from clause and note the estimated bytes. Now change th...
- **[cohorts/2023/week_3_data_warehouse/homework.md](./cohorts/2023/week_3_data_warehouse/homework.md_docs.md)**: Use the BQ table you created earlier in your from clause and note the estimated bytes. Now change th...
- **[cohorts/2025/03-data-warehouse/homework.md](./cohorts/2025/03-data-warehouse/homework.md_docs.md)**: terialized table you created earlier in your from clause and note the estimated bytes. Now change th...

## collection

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: ically manage the entire **data lifecycle**, from collection to consumption.

## collections

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: from collections import defaultdict
- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "from collections import namedtuple"

## collects

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: **Regular function collects data in memory.** Here you can see how data is co

## com

- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: import com.opencsv.CSVReader;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java_docs.md)**: import com.opencsv.exceptions.CsvException;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: import com.opencsv.CSVReader;

## companies

- **[awesome-data-engineering.md](./awesome-data-engineering.md_docs.md)**: Conference talks from companies, blog posts, etc

## compute_fingerprint

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def compute_fingerprint(self):

## compute_hash

- **[cohorts/2023/project.md](./cohorts/2023/project.md_docs.md)**: def compute_hash(email):

## concurrent

- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: from concurrent.futures import ThreadPoolExecutor

## confluent_kafka

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from confluent_kafka import Producer
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from confluent_kafka import Producer
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from confluent_kafka import Consumer

## consume_from_kafka

- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: def consume_from_kafka(self, topics: List[str]):
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: def consume_from_kafka(self, topics: List[str]):
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: def consume_from_kafka(self, topics: List[str]):
- **[06-streaming/python/streams-example/redpanda/consumer.py](./06-streaming/python/streams-example/redpanda/consumer.py_docs.md)**: def consume_from_kafka(self, topics: List[str]):
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: def consume_from_kafka(self, topics: List[str]):

## content

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: content = "\n".join(file_list)

## convertToInteractive

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-258adc47-8431-4a9d-903b-a075...
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-164dc4c0-056c-460d-b99f-0582...

## crashing

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: To prevent your pipeline from crashing, you need to control memory usage.

## crazy_stuff

- **[05-batch/code/04_pyspark.ipynb](./05-batch/code/04_pyspark.ipynb_docs.md)**: "def crazy_stuff(base_num):\n",

## create_bucket

- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: def create_bucket(bucket_name):

## create_engine

- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: from sqlalchemy import create_engine
- **[01-docker-terraform/2_docker_sql/README.md](./01-docker-terraform/2_docker_sql/README.md_docs.md)**: from sqlalchemy import create_engine
- **[01-docker-terraform/2_docker_sql/pg-test-connection.ipynb](./01-docker-terraform/2_docker_sql/pg-test-connection.ipynb_docs.md)**: "from sqlalchemy import create_engine"
- **[01-docker-terraform/2_docker_sql/ingest_data.py](./01-docker-terraform/2_docker_sql/ingest_data.py_docs.md)**: from sqlalchemy import create_engine
- **[01-docker-terraform/2_docker_sql/upload-data.ipynb](./01-docker-terraform/2_docker_sql/upload-data.ipynb_docs.md)**: "from sqlalchemy import create_engine"
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb](./01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb_docs.md)**: "from sqlalchemy import create_engine\n",
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py_docs.md)**: from sqlalchemy import create_engine

## create_events_aggregated_sink

- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: def create_events_aggregated_sink(t_env):

## create_events_source_kafka

- **[06-streaming/pyflink/src/job/taxi_job.py](./06-streaming/pyflink/src/job/taxi_job.py_docs.md)**: def create_events_source_kafka(t_env):
- **[06-streaming/pyflink/src/job/start_job.py](./06-streaming/pyflink/src/job/start_job.py_docs.md)**: def create_events_source_kafka(t_env):
- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: def create_events_source_kafka(t_env):

## create_processed_events_sink_postgres

- **[06-streaming/pyflink/src/job/start_job.py](./06-streaming/pyflink/src/job/start_job.py_docs.md)**: def create_processed_events_sink_postgres(t_env):

## create_readme

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def create_readme(self):

## create_taxi_events_sink_postgres

- **[06-streaming/pyflink/src/job/taxi_job.py](./06-streaming/pyflink/src/job/taxi_job.py_docs.md)**: def create_taxi_events_sink_postgres(t_env):

## csv

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: import csv
- **[06-streaming/pyflink/src/producers/load_taxi_data.py](./06-streaming/pyflink/src/producers/load_taxi_data.py_docs.md)**: import csv
- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: import csv
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: import csv
- **[06-streaming/python/streams-example/faust/producer_taxi_json.py](./06-streaming/python/streams-example/faust/producer_taxi_json.py_docs.md)**: import csv
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: import csv
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: import csv
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: import csv

## current_event

- **[06-streaming/python/streams-example/faust/branch_price.py](./06-streaming/python/streams-example/faust/branch_price.py_docs.md)**: from faust import current_event

## daemon

- **[01-docker-terraform/2_docker_sql/README.md](./01-docker-terraform/2_docker_sql/README.md_docs.md)**: docker: Error response from daemon: invalid mode: \Program Files\Git\var\lib\postgre

## data

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: y that you can add to your Python scripts to load data from various and often messy data sources int...

## dataTable

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " const dataTable =\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " const dataTable =\n",

## data_table

- **[cohorts/2025/workshops/dlt/dlt_homework.md](./cohorts/2025/workshops/dlt/dlt_homework.md_docs.md)**: from google.colab import data_table

## datatalks

- **[cohorts/2024/leaderboard.md](./cohorts/2024/leaderboard.md_docs.md)**: This is the top [100 leaderboard](https://courses.datatalks.club/de-zoomcamp-2024/leaderboard)

## datetime

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: from datetime import datetime
- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "from datetime import datetime"
- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from datetime import datetime
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from datetime import datetime
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py_docs.md)**: from datetime import datetime
- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: from datetime import datetime
- **[06-streaming/python/streams-example/faust/windowing.py](./06-streaming/python/streams-example/faust/windowing.py_docs.md)**: from datetime import timedelta
- **[06-streaming/python/redpanda_example/ride.py](./06-streaming/python/redpanda_example/ride.py_docs.md)**: from datetime import datetime

## days_ago

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from airflow.utils.dates import days_ago
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from airflow.utils.dates import days_ago
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: from airflow.utils.dates import days_ago
- **[cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py](./cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py_docs.md)**: from airflow.utils.dates import days_ago

## decimal

- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: from decimal import Decimal
- **[06-streaming/python/redpanda_example/ride.py](./06-streaming/python/redpanda_example/ride.py_docs.md)**: from decimal import Decimal

## defaultdict

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: from collections import defaultdict

## delivery_report

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: def delivery_report(err, msg):
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: def delivery_report(err, msg):
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: def delivery_report(err, msg):

## departureTime

- **[06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java](./06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java_docs.md)**: var departureTime = LocalDateTime.now().minusMinutes(30).format(Dat

## dict_to_ride_record

- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: def dict_to_ride_record(obj, ctx):
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from ride_record import dict_to_ride_record

## dict_to_ride_record_key

- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from ride_record_key import dict_to_ride_record_key
- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: def dict_to_ride_record_key(obj, ctx):

## dlt

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "# **Install `dlt`â³**"
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "!pip install dlt[duckdb]"
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: import dlt
- **[cohorts/2025/workshops/dlt/dlt_homework.md](./cohorts/2025/workshops/dlt/dlt_homework.md_docs.md)**: # **Workshop "Data Ingestion with dlt": Homework**
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: # Data ingestion with dlt
- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "!pip install dlt[bigquery, gs]"

## docLink

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " const docLink = document.createElement('div');\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " const docLink = document.createElement('div');\n",

## docLinkHtml

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " const docLinkHtml = 'Like what you see? Visit the ' +\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " const docLinkHtml = 'Like what you see? Visit the ' +\n",

## does

- **[04-analytics-engineering/SQL_refresher.md](./04-analytics-engineering/SQL_refresher.md_docs.md)**: lar aggregate functions, use of a window function does not cause rows to become grouped into a singl...

## donwload_parquetize_upload_dag

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: def donwload_parquetize_upload_dag(

## download_and_read_jsonl

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "def download_and_read_jsonl(url):\n",
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: def download_and_read_jsonl(url):

## download_and_yield_rows

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: def download_and_yield_rows(url):

## download_file

- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: def download_file(month):

## download_parquet

- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "def download_parquet():\n",

## dtc_zoomcamp_2025

- **[cohorts/2025/04-analytics-engineering/homework.md](./cohorts/2025/04-analytics-engineering/homework.md_docs.md)**: database: "{{ env_var('DBT_BIGQUERY_PROJECT', 'dtc_zoomcamp_2025') }}"

## duckdb

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "!pip install dlt[duckdb] # Install dlt with all the necessary DuckDB depe
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: 2 generators. You will be tasked to load them to duckdb and answer some questions from the data\n",
- **[cohorts/2025/workshops/dlt/dlt_homework.md](./cohorts/2025/workshops/dlt/dlt_homework.md_docs.md)**: !pip install dlt[duckdb]
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: /dlthub.com/docs/reference/installation) dlt with DuckDB as destination:
- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "!pip install dlt[duckdb]"

## dumps

- **[06-streaming/python/streams-example/faust/producer_taxi_json.py](./06-streaming/python/streams-example/faust/producer_taxi_json.py_docs.md)**: from json import dumps

## each

- **[projects/README.md](./projects/README.md_docs.md)**: This is a great opportunity for you to learn from each other.

## element

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " const element = document.querySelector('#df-258adc47-8431-4a9d-
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " const element = document.querySelector('#df-164dc4c0-056c-460d-

## event

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: Streaming here refers to processing the data event by event or chunk by chunk instead of doing bulk

## expected

- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java_docs.md)**: var expected = new VendorInfo(ride.VendorID, pickupLocation.PU

## external

- **[cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py](./cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py_docs.md)**: # Create a partitioned table from external table
- **[03-data-warehouse/big_query.sql](./03-data-warehouse/big_query.sql_docs.md)**: -- Creating external table referring to gcs path

## extract_keywords

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def extract_keywords(self, content, filepath):

## faust

- **[06-streaming/python/streams-example/faust/taxi_rides.py](./06-streaming/python/streams-example/faust/taxi_rides.py_docs.md)**: import faust
- **[06-streaming/python/streams-example/faust/stream.py](./06-streaming/python/streams-example/faust/stream.py_docs.md)**: import faust
- **[06-streaming/python/streams-example/faust/windowing.py](./06-streaming/python/streams-example/faust/windowing.py_docs.md)**: import faust
- **[06-streaming/python/streams-example/faust/branch_price.py](./06-streaming/python/streams-example/faust/branch_price.py_docs.md)**: import faust
- **[06-streaming/python/streams-example/faust/stream_count_vendor_trips.py](./06-streaming/python/streams-example/faust/stream_count_vendor_trips.py_docs.md)**: import faust

## file

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: .all_keywords = defaultdict(list) # keyword -> [(file, description)]
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb](./01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb_docs.md)**: "Here we will be using the ```.paraquet``` file we downloaded and do the following:\n",
- **[04-analytics-engineering/dbt_cloud_setup.md](./04-analytics-engineering/dbt_cloud_setup.md_docs.md)**: order to connect we need the service account JSON file generated from bigquery:
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: - Sometimes the API returns a secure file path to something like a json or parquet file in

## filesystem

- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: from dlt.sources.filesystem import filesystem, read_parquet
- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "from dlt.destinations import filesystem\n",

## filter_outliers

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "def filter_outliers(row):\n",

## finalize_manifest

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def finalize_manifest(self):

## find_related_files

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: {self.find_related_files(filepath, content)}

## following

- **[04-analytics-engineering/SQL_refresher.md](./04-analytics-engineering/SQL_refresher.md_docs.md)**: n often be useful to compare rows to preceding or following rows. You can use LAG or LEAD to create ...

## format_to_parquet

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: def format_to_parquet(src_file, dest_file):
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: def format_to_parquet(src_file):

## from_dict

- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: def from_dict(cls, d: Dict):
- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: def from_dict(cls, d: Dict):
- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: def from_dict(cls, d: Dict):
- **[06-streaming/python/redpanda_example/ride.py](./06-streaming/python/redpanda_example/ride.py_docs.md)**: def from_dict(cls, d: Dict):

## froukje

- **[01-docker-terraform/README.md](./01-docker-terraform/README.md_docs.md)**: * [Notes from froukje](https://github.com/froukje/de-zoomcamp/blob/main/week_1_basics_n_setup/notes
- **[cohorts/2023/week_2_workflow_orchestration/README.md](./cohorts/2023/week_2_workflow_orchestration/README.md_docs.md)**: * [Notes from froukje](https://github.com/froukje/de-zoomcamp/blob/main/week_2_workflow_orchestrati

## functions

- **[05-batch/code/06_spark_sql.py](./05-batch/code/06_spark_sql.py_docs.md)**: from pyspark.sql import functions as F
- **[05-batch/code/04_pyspark.ipynb](./05-batch/code/04_pyspark.ipynb_docs.md)**: "from pyspark.sql import functions as F"
- **[05-batch/code/06_spark_sql.ipynb](./05-batch/code/06_spark_sql.ipynb_docs.md)**: "from pyspark.sql import functions as F"
- **[05-batch/code/homework.ipynb](./05-batch/code/homework.ipynb_docs.md)**: "from pyspark.sql import functions as F"
- **[05-batch/code/06_spark_sql_big_query.py](./05-batch/code/06_spark_sql_big_query.py_docs.md)**: from pyspark.sql import functions as F
- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: from pyspark.sql import functions as F
- **[cohorts/2025/05-batch/homework/solution.ipynb](./cohorts/2025/05-batch/homework/solution.ipynb_docs.md)**: "from pyspark.sql import functions as F"

## generate_binary_file_doc

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def generate_binary_file_doc(self, filepath):

## generate_detailed_analysis

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: {self.generate_detailed_analysis(filepath, content)}

## generate_file_docs

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def generate_file_docs(self, filepath):

## generate_file_keywords

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def generate_file_keywords(self, filepath):

## generate_folder_docs

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def generate_folder_docs(self, folder_path):

## generate_folder_keywords

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: self.generate_folder_keywords(folder_path)

## generate_overview

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: {self.generate_overview(filepath, content)}

## generate_perf_security_notes

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: {self.generate_perf_security_notes(filepath, content)}

## generate_testing_notes

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: {self.generate_testing_notes(filepath, content)}

## generate_urls

- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: def generate_urls(color, start_year, end_year, start_month, end_mon

## generate_usage_examples

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: {self.generate_usage_examples(filepath, content)}

## generators

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: do the loading, which will process data from the generators incrementally, following the same memory...
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: ### Streaming in python via generators
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "Below you have 2 generators. You will be tasked to load them to duckdb and an

## get_commit_sha

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def get_commit_sha(self):

## get_keyword_context

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: kw_content += f"- **Context**: {self.get_keyword_context(kw, content)}\n\n"

## get_language_for_file

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: ```{self.get_language_for_file(filepath)}

## glob

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "import glob\n",

## google

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from google.cloud import storage
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from google.cloud import storage
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: from google.cloud import storage
- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " google.colab.kernel.accessAllowed ? 'block' : 'none';\n"
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: from google.cloud import storage
- **[cohorts/2025/workshops/dlt/dlt_homework.md](./cohorts/2025/workshops/dlt/dlt_homework.md_docs.md)**: https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: uery, Snowflake, Redshift)** or **data lakes (S3, Google Cloud Storage, Parquet files)** in dlt is i...
- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: from google.cloud import storage
- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "from google.colab import userdata\n",
- **[03-data-warehouse/extras/web_to_gcs.py](./03-data-warehouse/extras/web_to_gcs.py_docs.md)**: from google.cloud import storage

## green_taxi_trips

- **[cohorts/2025/01-docker-terraform/solution.md](./cohorts/2025/01-docker-terraform/solution.md_docs.md)**: green_taxi_trips

## green_tripdata

- **[04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql](./04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql_docs.md)**: with green_tripdata as (

## hashlib

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: import hashlib
- **[cohorts/2023/project.md](./cohorts/2023/project.md_docs.md)**: from hashlib import sha1

## healthcare

- **[projects/datasets.md](./projects/datasets.md_docs.md)**: e-collection-data-repositories-part-2.html) (from healthcare to transportation)

## helping

- **[asking-questions.md](./asking-questions.md_docs.md)**: OT tag instructors, it may discourage others from helping you.

## here

- **[certificates.md](./certificates.md_docs.md)**: [Adapted from here](https://support.edx.org/hc/en-us/articles/206501
- **[05-batch/setup/windows.md](./05-batch/setup/windows.md_docs.md)**: Here we'll show you how to install Spark 3.3.2 for Win
- **[cohorts/2023/leaderboard.md](./cohorts/2023/leaderboard.md_docs.md)**: so much for the course. Learn so many thing from here.</details></td>

## highest

- **[04-analytics-engineering/SQL_refresher.md](./04-analytics-engineering/SQL_refresher.md_docs.md)**: The query returns the top 10 highest total_amount values from the table, along with a

## http

- **[04-analytics-engineering/taxi_rides_ny/README.md](./04-analytics-engineering/taxi_rides_ny/README.md_docs.md)**: on a webserver, but it can also be accessed from http://localhost:8080 : `$ dbt docs serve`
- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: ontainer starts, you can access the Kestra UI at [http://localhost:8080](http://localhost:8080).

## https

- **[03-data-warehouse/extras/README.md](./03-data-warehouse/extras/README.md_docs.md)**: to GCS, without Airflow. Downloads csv files from https://nyc-tlc.s3.amazonaws.com/trip+data/ and up...

## infer_folder_purpose

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: {self.infer_folder_purpose(folder_path)}

## ingest_callable

- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py_docs.md)**: from ingest_script import ingest_callable
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py_docs.md)**: def ingest_callable(user, password, host, port, db, table_name, csv_f

## ingest_script

- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py_docs.md)**: from ingest_script import ingest_callable

## installation

- **[05-batch/setup/macos.md](./05-batch/setup/macos.md_docs.md)**: are having anaconda setup, you can skip the spark installation and instead Pyspark package to run th...

## is_binary_file

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def is_binary_file(self, filepath):

## its

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: """Infer the purpose of a folder from its name and contents"""
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: They ensure data flows seamlessly from its source to its final destination, where it can dri

## java

- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java_docs.md)**: import java.util.Properties;
- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java_docs.md)**: import java.util.Properties;
- **[06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java](./06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java_docs.md)**: import java.time.LocalDateTime;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: import java.time.Duration;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: import java.util.Properties;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: import java.time.Duration;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java_docs.md)**: import java.time.Duration;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: import java.io.FileReader;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java_docs.md)**: import java.io.IOException;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: import java.io.FileReader;
- *(... and 4 more occurrences)*

## javax

- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java_docs.md)**: import javax.xml.crypto.Data;

## joined

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: ig.APPLICATION_ID_CONFIG, "kafka_tutorial.kstream.joined.rides.pickuplocation.v1");

## json

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: import json
- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: xample, we created a simple http api that returns json \"page by page\", 1000 records per page.\n",
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: y typed and without explicit schema, such as csv, json
- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: import json
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: import json
- **[cohorts/2025/06-streaming/homework.md](./cohorts/2025/06-streaming/homework.md_docs.md)**: import json
- **[cohorts/2025/06-streaming/homework/homework.ipynb](./cohorts/2025/06-streaming/homework/homework.ipynb_docs.md)**: "import json\n",
- **[06-streaming/pyflink/homework.md](./06-streaming/pyflink/homework.md_docs.md)**: import json
- **[06-streaming/pyflink/src/producers/producer.py](./06-streaming/pyflink/src/producers/producer.py_docs.md)**: import json
- **[06-streaming/pyflink/src/producers/load_taxi_data.py](./06-streaming/pyflink/src/producers/load_taxi_data.py_docs.md)**: import json
- *(... and 6 more occurrences)*

## json_serializer

- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: def json_serializer(data):
- **[cohorts/2025/06-streaming/homework.md](./cohorts/2025/06-streaming/homework.md_docs.md)**: def json_serializer(data):
- **[cohorts/2025/06-streaming/homework/homework.ipynb](./cohorts/2025/06-streaming/homework/homework.ipynb_docs.md)**: "def json_serializer(data):\n",
- **[06-streaming/pyflink/homework.md](./06-streaming/pyflink/homework.md_docs.md)**: def json_serializer(data):
- **[06-streaming/pyflink/src/producers/producer.py](./06-streaming/pyflink/src/producers/producer.py_docs.md)**: def json_serializer(data):

## kStreams

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: var kStreams = new KafkaStreams(topology, props);
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: var kStreams = new KafkaStreams(topology, props);
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: var kStreams = new KafkaStreams(topology, props);

## kafka

- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: Instead of Kafka, we will use Red Panda, which is a drop-in
- **[cohorts/2025/06-streaming/homework.md](./cohorts/2025/06-streaming/homework.md_docs.md)**: Instead of Kafka, we will use Red Panda, which is a drop-in
- **[cohorts/2025/06-streaming/homework/homework.ipynb](./cohorts/2025/06-streaming/homework/homework.ipynb_docs.md)**: "from kafka import KafkaProducer\n",
- **[06-streaming/pyflink/homework.md](./06-streaming/pyflink/homework.md_docs.md)**: ## Question 1. Connecting to the Kafka server
- **[06-streaming/pyflink/src/producers/producer.py](./06-streaming/pyflink/src/producers/producer.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/pyflink/src/producers/load_taxi_data.py](./06-streaming/pyflink/src/producers/load_taxi_data.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from kafka import KafkaConsumer
- **[06-streaming/python/streams-example/faust/producer_taxi_json.py](./06-streaming/python/streams-example/faust/producer_taxi_json.py_docs.md)**: from kafka import KafkaProducer
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: from kafka import KafkaProducer
- *(... and 5 more occurrences)*

## load_schema

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: key_schema_str = self.load_schema(props['schema.key'])
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: key_schema_str = self.load_schema(props['schema.key'])

## loads

- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from json import loads
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: from json import loads

## local

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: - from APIs to local files;

## log_aggregation

- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: def log_aggregation():

## log_processing

- **[06-streaming/pyflink/src/job/taxi_job.py](./06-streaming/pyflink/src/job/taxi_job.py_docs.md)**: def log_processing():
- **[06-streaming/pyflink/src/job/start_job.py](./06-streaming/pyflink/src/job/start_job.py_docs.md)**: def log_processing():

## logging

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: import logging
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: import logging
- **[cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py](./cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py_docs.md)**: import logging

## looks

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: mple of a regular function, and how that function looks if written as a generator.

## main

- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: def main(params):
- **[01-docker-terraform/2_docker_sql/ingest_data.py](./01-docker-terraform/2_docker_sql/ingest_data.py_docs.md)**: def main(params):
- **[06-streaming/pyflink/src/producers/load_taxi_data.py](./06-streaming/pyflink/src/producers/load_taxi_data.py_docs.md)**: def main():

## mimetypes

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: import mimetypes

## model_predict

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "def model_predict(df):\n",

## most

- **[01-docker-terraform/1_terraform_gcp/1_terraform_overview.md](./01-docker-terraform/1_terraform_gcp/1_terraform_overview.md_docs.md)**: in directory of publicly available providers from most major infrastructure platforms.

## myproject

- **[cohorts/2025/04-analytics-engineering/homework.md](./cohorts/2025/04-analytics-engineering/homework.md_docs.md)**: export DBT_BIGQUERY_PROJECT=myproject

## name

- **[06-streaming/java/kafka_examples/gradlew](./06-streaming/java/kafka_examples/gradlew_docs.md)**: bash, then to run this script, type that shell name before the whole

## namedtuple

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "from collections import namedtuple"

## nested

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: s: CSV, JSON, where fields might be inconsistent, nested or missing key details.

## nth_yielded_number

- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "def nth_yielded_number(generator, n):\n",

## ny_taxi

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: def ny_taxi():

## object

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: var object = new JsonKStreamWindow();
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: var object = new JsonKStream();
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: var object = new JsonKStreamJoins();

## offical

- **[06-streaming/python/docker/spark/cluster-base.Dockerfile](./06-streaming/python/docker/spark/cluster-base.Dockerfile_docs.md)**: # Reference from offical Apache Spark repository Dockerfile for Kubernetes

## official

- **[04-analytics-engineering/README.md](./04-analytics-engineering/README.md_docs.md)**: tdbt.com/guides/bigquery?step=4)) | - follow the [official dbt documentation]([https://docs.getdbt.c...

## only

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: your machine? In the case of data extraction, the only limits are memory and storage. This refers to...

## op_groupby

- **[cohorts/2023/week_6_stream_processing/streaming_confluent.py](./cohorts/2023/week_6_stream_processing/streaming_confluent.py_docs.md)**: def op_groupby(df, column_names):
- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def op_groupby(df, column_names):
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def op_groupby(df, column_names):

## op_windowed_groupby

- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def op_windowed_groupby(df, window_duration, slide_duration):
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def op_windowed_groupby(df, window_duration, slide_duration):

## org

- **[06-streaming/python/docker/spark/spark-master.Dockerfile](./06-streaming/python/docker/spark/spark-master.Dockerfile_docs.md)**: CMD bin/spark-class org.apache.spark.deploy.master.Master >> logs/spark-m
- **[06-streaming/python/docker/spark/spark-worker.Dockerfile](./06-streaming/python/docker/spark/spark-worker.Dockerfile_docs.md)**: CMD bin/spark-class org.apache.spark.deploy.worker.Worker spark://${SPARK
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java_docs.md)**: import org.apache.avro.generic.GenericArray;
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java_docs.md)**: import org.apache.avro.generic.GenericArray;
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java_docs.md)**: import org.apache.avro.generic.GenericArray;
- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java_docs.md)**: package org.example;
- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java_docs.md)**: package org.example;
- **[06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java](./06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java_docs.md)**: package org.example.helper;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: package org.example;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: package org.example;
- *(... and 6 more occurrences)*

## other

- **[projects/README.md](./projects/README.md_docs.md)**: wn in the course (Data Studio or Metabase) or any other BI tool of your choice to build a dashboard....
- **[04-analytics-engineering/SQL_refresher.md](./04-analytics-engineering/SQL_refresher.md_docs.md)**: G or LEAD to create columns that pull values from other rows without the need for a self-join. All y...
- **[cohorts/2024/02-workflow-orchestration/README.md](./cohorts/2024/02-workflow-orchestration/README.md_docs.md)**: tform. We'll cover what makes Mage different from other orchestrators, the fundamental concepts behi...

## others

- **[projects/README.md](./projects/README.md_docs.md)**: atures will definitely help you to stand out from others.

## our

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: Hereâ€™s how we design our requester:
- **[06-streaming/python/redpanda_example/README.md](./06-streaming/python/redpanda_example/README.md_docs.md)**: t the following `rpk` alias so we can use it from our terminal, without having to open a Docker inte...

## over

- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: Iterate over the records in the dataframe

## page

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: amiliar with. The API returns **1,000 records per page**, and we must request multiple pages to retr...

## paginated_getter

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "def paginated_getter():\n",
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: def paginated_getter():
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: def paginated_getter():
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: def paginated_getter():

## pandas

- **[05-batch/README.md](./05-batch/README.md_docs.md)**: > The other way to infer the schema (apart from pandas) for the csv files, is to set the `inferSchem...
- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "import pandas as pd"
- **[05-batch/code/04_pyspark.ipynb](./05-batch/code/04_pyspark.ipynb_docs.md)**: "import pandas as pd"
- **[05-batch/code/05_taxi_schema.ipynb](./05-batch/code/05_taxi_schema.ipynb_docs.md)**: "import pandas as pd"
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: import pandas as pd
- **[01-docker-terraform/2_docker_sql/README.md](./01-docker-terraform/2_docker_sql/README.md_docs.md)**: eps to read ```.parquet``` file and convert it to Pandas data frame.
- **[01-docker-terraform/2_docker_sql/pg-test-connection.ipynb](./01-docker-terraform/2_docker_sql/pg-test-connection.ipynb_docs.md)**: "import pandas as pd"
- **[01-docker-terraform/2_docker_sql/pipeline.py](./01-docker-terraform/2_docker_sql/pipeline.py_docs.md)**: import pandas as pd
- **[01-docker-terraform/2_docker_sql/ingest_data.py](./01-docker-terraform/2_docker_sql/ingest_data.py_docs.md)**: import pandas as pd
- **[01-docker-terraform/2_docker_sql/upload-data.ipynb](./01-docker-terraform/2_docker_sql/upload-data.ipynb_docs.md)**: "import pandas as pd"
- *(... and 6 more occurrences)*

## parquet

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "## Load to parquet file"

## parquet_source

- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: def parquet_source():

## parse_ride_from_kafka_message

- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def parse_ride_from_kafka_message(df, schema):
- **[06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb](./06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb_docs.md)**: "def parse_ride_from_kafka_message(df_raw, schema):\n",
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def parse_ride_from_kafka_message(df, schema):
- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: "def parse_ride_from_kafka_message(df_raw, schema):\n",

## parse_rides

- **[cohorts/2023/week_6_stream_processing/streaming_confluent.py](./cohorts/2023/week_6_stream_processing/streaming_confluent.py_docs.md)**: def parse_rides(df, schema):

## parse_row

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: def parse_row(self, row):

## pathlib

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: from pathlib import Path

## payment_type_sessions

- **[06-streaming/ksqldb/commands.md](./06-streaming/ksqldb/commands.md_docs.md)**: CREATE TABLE payment_type_sessions AS

## peatwan

- **[cohorts/2025/workshops/dlt/README.md](./cohorts/2025/workshops/dlt/README.md_docs.md)**: * [Ingest Data to GCS by dlt from peatwan](https://github.com/peatwan/de-zoomcamp/tree/main

## peek

- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: def peek(mini_batch, batch_id):

## people_1

- **[cohorts/2024/workshops/dlt_resources/homework_starter.ipynb](./cohorts/2024/workshops/dlt_resources/homework_starter.ipynb_docs.md)**: "def people_1():\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "def people_1():\n",

## people_2

- **[cohorts/2024/workshops/dlt_resources/homework_starter.ipynb](./cohorts/2024/workshops/dlt_resources/homework_starter.ipynb_docs.md)**: "def people_2():\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "def people_2():\n",

## performs

- **[04-analytics-engineering/SQL_refresher.md](./04-analytics-engineering/SQL_refresher.md_docs.md)**: A window function performs a calculation across a set of table rows that are

## period

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: var period = Duration.between(ride.tpep_dropoff_datetime, pi

## pickupLocationsKeyedOnPUId

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: var pickupLocationsKeyedOnPUId = pickupLocations.selectKey((key, value) -> Strin

## pickup_datetime

- **[cohorts/2025/04-analytics-engineering/homework.md](./cohorts/2025/04-analytics-engineering/homework.md_docs.md)**: where pickup_datetime >= CURRENT_DATE - INTERVAL '30' DAY

## prepare_dataframe_to_kafka_sink

- **[06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb](./06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb_docs.md)**: "def prepare_dataframe_to_kafka_sink(df, value_columns, key_column=None):\n",
- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: "def prepare_dataframe_to_kafka_sink(df, value_columns, key_column=None):\n",

## prepare_df_to_kafka_sink

- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def prepare_df_to_kafka_sink(df, value_columns, key_column=None):
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def prepare_df_to_kafka_sink(df, value_columns, key_column=None):

## prepare_for_grouping

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "def prepare_for_grouping(row): \n",

## previous

- **[projects/README.md](./projects/README.md_docs.md)**: * Re-using your ML Zoomcamp from previous iterations of the course
- **[04-analytics-engineering/SQL_refresher.md](./04-analytics-engineering/SQL_refresher.md_docs.md)**: - LAG(): Retrieves the value from a previous row.

## process

- **[06-streaming/python/streams-example/faust/windowing.py](./06-streaming/python/streams-example/faust/windowing.py_docs.md)**: async def process(stream):
- **[06-streaming/python/streams-example/faust/branch_price.py](./06-streaming/python/streams-example/faust/branch_price.py_docs.md)**: async def process(stream):
- **[06-streaming/python/streams-example/faust/stream_count_vendor_trips.py](./06-streaming/python/streams-example/faust/stream_count_vendor_trips.py_docs.md)**: async def process(stream):

## process_all_files

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def process_all_files(self):

## process_all_folders

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def process_all_folders(self):

## processed_events

- **[cohorts/2025/06-streaming/homework.md](./cohorts/2025/06-streaming/homework.md_docs.md)**: CREATE TABLE processed_events (
- **[06-streaming/pyflink/homework.md](./06-streaming/pyflink/homework.md_docs.md)**: CREATE TABLE processed_events (

## processed_events_aggregated

- **[cohorts/2025/06-streaming/homework.md](./cohorts/2025/06-streaming/homework.md_docs.md)**: CREATE TABLE processed_events_aggregated (

## producer

- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: import org.apache.kafka.clients.producer.KafkaProducer;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java_docs.md)**: import org.apache.kafka.clients.producer.KafkaProducer;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: import org.apache.kafka.clients.producer.*;

## puLocationCount

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: var puLocationCount = ridesStream.groupByKey()
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: var puLocationCount = ridesStream.groupByKey().count().toStream();

## publish

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: def publish(self, records: [str, str], topic: str):
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: def publish(self, topic: str, records: [RideRecordKey, RideRe
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: def publish(self, topic: str, records: [str, str]):
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: def publish(self, topic: str, records: [str, str]):

## publish_rides

- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: def publish_rides(self, topic: str, messages: List[Ride]):
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: def publish_rides(self, topic: str, messages: List[Ride]):

## pyarrow

- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: import pyarrow.parquet as pq
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb](./01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb_docs.md)**: "import pyarrow.parquet as pq\n",
- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: import pyarrow.csv as pv
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: import pyarrow.csv as pv
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: import pyarrow.parquet as pq

## pyflink

- **[06-streaming/pyflink/src/job/taxi_job.py](./06-streaming/pyflink/src/job/taxi_job.py_docs.md)**: from pyflink.datastream import StreamExecutionEnvironment
- **[06-streaming/pyflink/src/job/start_job.py](./06-streaming/pyflink/src/job/start_job.py_docs.md)**: from pyflink.datastream import StreamExecutionEnvironment
- **[06-streaming/pyflink/src/job/aggregation_job.py](./06-streaming/pyflink/src/job/aggregation_job.py_docs.md)**: from pyflink.datastream import StreamExecutionEnvironment

## pyspark

- **[05-batch/code/06_spark_sql.py](./05-batch/code/06_spark_sql.py_docs.md)**: import pyspark
- **[05-batch/code/09_spark_gcs.ipynb](./05-batch/code/09_spark_gcs.ipynb_docs.md)**: "import pyspark\n",
- **[05-batch/code/03_test.ipynb](./05-batch/code/03_test.ipynb_docs.md)**: "import pyspark"
- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "import pyspark\n",
- **[05-batch/code/04_pyspark.ipynb](./05-batch/code/04_pyspark.ipynb_docs.md)**: "import pyspark\n",
- **[05-batch/code/06_spark_sql.ipynb](./05-batch/code/06_spark_sql.ipynb_docs.md)**: "import pyspark\n",
- **[05-batch/code/homework.ipynb](./05-batch/code/homework.ipynb_docs.md)**: "import pyspark\n",
- **[05-batch/code/07_groupby_join.ipynb](./05-batch/code/07_groupby_join.ipynb_docs.md)**: "import pyspark\n",
- **[05-batch/code/05_taxi_schema.ipynb](./05-batch/code/05_taxi_schema.ipynb_docs.md)**: "import pyspark\n",
- **[05-batch/code/06_spark_sql_big_query.py](./05-batch/code/06_spark_sql_big_query.py_docs.md)**: import pyspark
- *(... and 12 more occurrences)*

## quickchart

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0744a05-797a-4214-b06
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d353cda7-9937-430a-a4e

## quickchartButtonEl

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: " const quickchartButtonEl =\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: " const quickchartButtonEl =\n",

## read_ccloud_config

- **[cohorts/2023/week_6_stream_processing/settings.py](./cohorts/2023/week_6_stream_processing/settings.py_docs.md)**: def read_ccloud_config(config_file):

## read_from_kafka

- **[cohorts/2023/week_6_stream_processing/streaming_confluent.py](./cohorts/2023/week_6_stream_processing/streaming_confluent.py_docs.md)**: def read_from_kafka(consume_topic: str):
- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def read_from_kafka(consume_topic: str):
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def read_from_kafka(consume_topic: str):

## read_records

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: def read_records(self, resource_path: str):
- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: def read_records(resource_path: str):
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: def read_records(resource_path: str):
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: def read_records(resource_path: str):
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: def read_records(resource_path: str):
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: def read_records(resource_path: str):

## reader

- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: var reader = new CSVReader(new FileReader(ridesStream.getFil
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: var reader = new CSVReader(new FileReader(ridesStream.getFil

## record

- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: var record = kafkaProducer.send(new ProducerRecord<>("rides_
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java_docs.md)**: var record = kafkaProducer.send(new ProducerRecord<>("rides_
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: var record = kafkaProducer.send(new ProducerRecord<>("rides"

## requests

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "import requests\n",
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: l they succeed. For example, dlt library offers a requests â€œreplacementâ€ that has built in retries. ...
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: import requests
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: rate limits**: APIs often restrict the number of requests you can make in a given time.
- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "import requests\n",
- **[03-data-warehouse/extras/web_to_gcs.py](./03-data-warehouse/extras/web_to_gcs.py_docs.md)**: import requests

## result

- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java_docs.md)**: var result = outputTopic.readKeyValue();

## results

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java_docs.md)**: var results = consumer.poll(Duration.of(1, ChronoUnit.SECONDS

## ride

- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from ride import Ride
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from ride import Ride
- **[06-streaming/python/redpanda_example/producer.py](./06-streaming/python/redpanda_example/producer.py_docs.md)**: from ride import Ride
- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: from ride import Ride

## rideRecords

- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: var rideRecords = producer.getRides();

## ride_record

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from ride_record import RideRecord, ride_record_to_dict
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from ride_record import dict_to_ride_record

## ride_record_key

- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from ride_record_key import RideRecordKey, ride_record_key_to_dict
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from ride_record_key import dict_to_ride_record_key

## ride_record_key_to_dict

- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: def ride_record_key_to_dict(ride_record_key: RideRecordKey, ctx):

## ride_record_to_dict

- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: def ride_record_to_dict(ride_record: RideRecord, ctx):

## rides

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: var ridesStream = this.getClass().getResource("/rides.csv");

## ridesStream

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: var ridesStream = streamsBuilder.stream("rides", Consumed.with(Se
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: var ridesStream = streamsBuilder.stream("rides", Consumed.with(Se
- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: var ridesStream = this.getClass().getResource("/rides.csv");
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java_docs.md)**: var ridesStream = this.getClass().getResource("/rides.csv");

## run

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: result = subprocess.run(['git', 'rev-parse', 'HEAD'],

## safe_read_file

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def safe_read_file(self, filepath):

## scan_repository

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def scan_repository(self):

## schema

- **[01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb](./01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb_docs.md)**: "# Read file, read the table from file and check schema\n",

## schemaregistry

- **[06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java](./06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java_docs.md)**: import schemaregistry.RideRecord;

## scratch

- **[README.md](./README.md_docs.md)**: ring by building an end-to-end data pipeline from scratch. Gain hands-on experience with industry-st...
- **[04-analytics-engineering/dbt_cloud_setup.md](./04-analytics-engineering/dbt_cloud_setup.md_docs.md)**: he sake of showing the creation of a project from scratch I've created a new empty repository just f...
- **[cohorts/2024/workshops/dlt.md](./cohorts/2024/workshops/dlt.md_docs.md)**: - we cannot all just build perfect pipelines from scratch every time.
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: lines much faster than if you did everything from scratch.

## search_twitter

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: def search_twitter(query):

## serde

- **[06-streaming/java/kafka_examples/src/main/java/org/example/customserdes/CustomSerdes.java](./06-streaming/java/kafka_examples/src/main/java/org/example/customserdes/CustomSerdes.java_docs.md)**: import org.apache.kafka.common.serialization.Serde;

## settings

- **[cohorts/2023/week_6_stream_processing/streaming_confluent.py](./cohorts/2023/week_6_stream_processing/streaming_confluent.py_docs.md)**: from settings import CONFLUENT_CLOUD_CONFIG, GREEN_TAXI_TOPIC,
- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from settings import CONFLUENT_CLOUD_CONFIG, \
- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, INPUT_DATA_PATH, KAFKA_
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, KAFKA_TOPIC
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from settings import RIDE_KEY_SCHEMA_PATH, RIDE_VALUE_SCHEMA_PA
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, SCHEMA_REGISTRY_URL, \
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, INPUT_DATA_PATH, PRODUC
- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: from settings import RIDE_SCHEMA, CONSUME_TOPIC_RIDES_CSV, TOPI
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, CONSUME_TOPIC_RIDES_CSV
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: from settings import BOOTSTRAP_SERVERS, INPUT_DATA_PATH, PRODUC
- *(... and 4 more occurrences)*

## sha1

- **[cohorts/2023/project.md](./cohorts/2023/project.md_docs.md)**: from hashlib import sha1

## sink_console

- **[cohorts/2023/week_6_stream_processing/streaming_confluent.py](./cohorts/2023/week_6_stream_processing/streaming_confluent.py_docs.md)**: def sink_console(df, output_mode: str = 'complete', processing_tim
- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def sink_console(df, output_mode: str = 'complete', processing_tim
- **[06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb](./06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb_docs.md)**: "def sink_console(df, output_mode: str = 'complete', processing_tim
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def sink_console(df, output_mode: str = 'complete', processing_tim
- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: "def sink_console(df, output_mode: str = 'complete', processing_tim

## sink_kafka

- **[cohorts/2023/week_6_stream_processing/streaming_confluent.py](./cohorts/2023/week_6_stream_processing/streaming_confluent.py_docs.md)**: def sink_kafka(df, topic, output_mode: str = 'complete'):
- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def sink_kafka(df, topic):
- **[06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb](./06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb_docs.md)**: "def sink_kafka(df, topic, output_mode='append'):\n",
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def sink_kafka(df, topic):
- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: "def sink_kafka(df, topic, output_mode='append'):\n",

## sink_memory

- **[06-streaming/python/streams-example/pyspark/streaming.py](./06-streaming/python/streams-example/pyspark/streaming.py_docs.md)**: def sink_memory(df, query_name, query_template):
- **[06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb](./06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb_docs.md)**: "def sink_memory(df, query_name, query_template):\n",
- **[06-streaming/python/streams-example/redpanda/streaming.py](./06-streaming/python/streams-example/redpanda/streaming.py_docs.md)**: def sink_memory(df, query_name, query_template):
- **[06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb](./06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb_docs.md)**: "def sink_memory(df, query_name, query_template):\n",

## sleep

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from time import sleep
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from time import sleep
- **[06-streaming/python/streams-example/faust/producer_taxi_json.py](./06-streaming/python/streams-example/faust/producer_taxi_json.py_docs.md)**: from time import sleep
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: from time import sleep
- **[06-streaming/python/streams-example/redpanda/producer.py](./06-streaming/python/streams-example/redpanda/producer.py_docs.md)**: from time import sleep

## source

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: engineerâ€™s main goal is to ensure data flows from source systems to analytical destinations.
- **[06-streaming/pyflink/Dockerfile.flink](./06-streaming/pyflink/Dockerfile.flink_docs.md)**: o 3.9 in Debian 11 and so install Python 3.7 from source

## sources

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: ts to load data from various and often messy data sources into well-structured, live datasets.\n",

## sqlalchemy

- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: from sqlalchemy import create_engine
- **[01-docker-terraform/2_docker_sql/README.md](./01-docker-terraform/2_docker_sql/README.md_docs.md)**: from sqlalchemy import create_engine
- **[01-docker-terraform/2_docker_sql/pg-test-connection.ipynb](./01-docker-terraform/2_docker_sql/pg-test-connection.ipynb_docs.md)**: "pip install sqlalchemy psycopg2-binary \n",
- **[01-docker-terraform/2_docker_sql/ingest_data.py](./01-docker-terraform/2_docker_sql/ingest_data.py_docs.md)**: from sqlalchemy import create_engine
- **[01-docker-terraform/2_docker_sql/upload-data.ipynb](./01-docker-terraform/2_docker_sql/upload-data.ipynb_docs.md)**: "from sqlalchemy import create_engine"
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb](./01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb_docs.md)**: " - Create a connection to our database using SQLAlchemy\n",
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py_docs.md)**: from sqlalchemy import create_engine

## square_root_generator

- **[cohorts/2024/workshops/dlt_resources/homework_starter.ipynb](./cohorts/2024/workshops/dlt_resources/homework_starter.ipynb_docs.md)**: "def square_root_generator(limit):\n",
- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "def square_root_generator(limit):\n",

## start_reading

- **[06-streaming/python/streams-example/faust/stream.py](./06-streaming/python/streams-example/faust/stream.py_docs.md)**: async def start_reading(records):

## static

- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java_docs.md)**: import static org.junit.jupiter.api.Assertions.*;
- **[06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java](./06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java_docs.md)**: import static org.junit.jupiter.api.Assertions.*;

## stg_fhv_tripdata

- **[cohorts/2024/04-analytics-engineering/homework.md](./cohorts/2024/04-analytics-engineering/homework.md_docs.md)**: * Building a source table: `stg_fhv_tripdata`

## storage

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: from google.cloud import storage
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: from google.cloud import storage
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: from google.cloud import storage
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: a data engineer may also focus on optimising data storage, ensuring data quality and integrity, impl...
- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: from google.cloud import storage
- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: from google.cloud import storage
- **[03-data-warehouse/extras/web_to_gcs.py](./03-data-warehouse/extras/web_to_gcs.py_docs.md)**: from google.cloud import storage

## stream_download_jsonl

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "def stream_download_jsonl(url):\n",

## subprocess

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: import subprocess

## sum_of_generator_outputs

- **[cohorts/2024/workshops/dlt_resources/homework_solution.ipynb](./cohorts/2024/workshops/dlt_resources/homework_solution.ipynb_docs.md)**: "def sum_of_generator_outputs(generator, limit):\n",

## sys

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: import sys
- **[01-docker-terraform/2_docker_sql/pipeline.py](./01-docker-terraform/2_docker_sql/pipeline.py_docs.md)**: import sys
- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: import sys

## taxi_rides

- **[06-streaming/python/streams-example/faust/stream.py](./06-streaming/python/streams-example/faust/stream.py_docs.md)**: from taxi_rides import TaxiRide
- **[06-streaming/python/streams-example/faust/windowing.py](./06-streaming/python/streams-example/faust/windowing.py_docs.md)**: from taxi_rides import TaxiRide
- **[06-streaming/python/streams-example/faust/branch_price.py](./06-streaming/python/streams-example/faust/branch_price.py_docs.md)**: from taxi_rides import TaxiRide
- **[06-streaming/python/streams-example/faust/stream_count_vendor_trips.py](./06-streaming/python/streams-example/faust/stream_count_vendor_trips.py_docs.md)**: from taxi_rides import TaxiRide

## text

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: data = response.text.splitlines()

## that

- **[05-batch/code/05_taxi_schema.ipynb](./05-batch/code/05_taxi_schema.ipynb_docs.md)**: \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001...
- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: "* dlt is an open-source library that you can add to your Python scripts to load data f
- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: - In that case, they can go straight to using it; Examples:
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java_docs.md)**: new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java_docs.md)**: new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
- **[06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java](./06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java_docs.md)**: new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.

## this

- **[projects/datasets.md](./projects/datasets.md_docs.md)**: It's not mandatory that you use a dataset from this list. You can use any dataset you want.
- **[cohorts/2022/week_2_data_ingestion/airflow/2_setup_nofrills.md](./cohorts/2022/week_2_data_ingestion/airflow/2_setup_nofrills.md_docs.md)**: 1. For the sake of standardization across this workshop's config,
- **[cohorts/2022/week_3_data_warehouse/airflow/2_setup_nofrills.md](./cohorts/2022/week_3_data_warehouse/airflow/2_setup_nofrills.md_docs.md)**: 1. For the sake of standardization across this workshop's config,

## time

- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: from time import time
- **[01-docker-terraform/2_docker_sql/ingest_data.py](./01-docker-terraform/2_docker_sql/ingest_data.py_docs.md)**: from time import time
- **[01-docker-terraform/2_docker_sql/upload-data.ipynb](./01-docker-terraform/2_docker_sql/upload-data.ipynb_docs.md)**: "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb](./01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb_docs.md)**: "from time import time"
- **[cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py](./cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py_docs.md)**: from time import time
- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: pt and structure data as it evolves, reducing the time spent on maintenance and development. This en...
- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: import time
- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from time import sleep
- **[cohorts/2025/06-streaming/homework.md](./cohorts/2025/06-streaming/homework.md_docs.md)**: Use `from time import time` to see the total time
- **[cohorts/2025/06-streaming/homework/homework.ipynb](./cohorts/2025/06-streaming/homework/homework.ipynb_docs.md)**: "from time import time"
- *(... and 7 more occurrences)*

## timedelta

- **[06-streaming/python/streams-example/faust/windowing.py](./06-streaming/python/streams-example/faust/windowing.py_docs.md)**: from datetime import timedelta

## toml

- **[cohorts/2025/workshops/dynamic_load_dlt.py](./cohorts/2025/workshops/dynamic_load_dlt.py_docs.md)**: import toml

## topology

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: import org.apache.kafka.streams.Topology;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java_docs.md)**: import org.apache.kafka.streams.Topology;
- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java_docs.md)**: import org.apache.kafka.streams.Topology;

## tqdm

- **[cohorts/2025/06-streaming/homework/homework.ipynb](./cohorts/2025/06-streaming/homework/homework.ipynb_docs.md)**: "from tqdm.auto import tqdm"

## tripdata

- **[04-analytics-engineering/taxi_rides_ny/models/staging/stg_green_tripdata.sql](./04-analytics-engineering/taxi_rides_ny/models/staging/stg_green_tripdata.sql_docs.md)**: with tripdata as
- **[04-analytics-engineering/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql](./04-analytics-engineering/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql_docs.md)**: with tripdata as

## trips_data

- **[04-analytics-engineering/taxi_rides_ny/models/core/dm_monthly_zone_revenue.sql](./04-analytics-engineering/taxi_rides_ny/models/core/dm_monthly_zone_revenue.sql_docs.md)**: with trips_data as (

## trips_unioned

- **[04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql](./04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql_docs.md)**: trips_unioned as (

## types

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "from pyspark.sql import types"
- **[05-batch/code/04_pyspark.ipynb](./05-batch/code/04_pyspark.ipynb_docs.md)**: "from pyspark.sql import types"
- **[05-batch/code/homework.ipynb](./05-batch/code/homework.ipynb_docs.md)**: "from pyspark.sql import types"
- **[05-batch/code/05_taxi_schema.ipynb](./05-batch/code/05_taxi_schema.ipynb_docs.md)**: "from pyspark.sql import types"
- **[cohorts/2024/06-streaming/homework.md](./cohorts/2024/06-streaming/homework.md_docs.md)**: from pyspark.sql import types
- **[cohorts/2025/05-batch/homework/solution.ipynb](./cohorts/2025/05-batch/homework/solution.ipynb_docs.md)**: "from pyspark.sql import types\n",

## typing

- **[cohorts/2023/week_6_stream_processing/producer_confluent.py](./cohorts/2023/week_6_stream_processing/producer_confluent.py_docs.md)**: from typing import Dict
- **[06-streaming/python/json_example/producer.py](./06-streaming/python/json_example/producer.py_docs.md)**: from typing import List, Dict
- **[06-streaming/python/json_example/ride.py](./06-streaming/python/json_example/ride.py_docs.md)**: from typing import List, Dict
- **[06-streaming/python/json_example/consumer.py](./06-streaming/python/json_example/consumer.py_docs.md)**: from typing import Dict, List
- **[06-streaming/python/avro_example/producer.py](./06-streaming/python/avro_example/producer.py_docs.md)**: from typing import Dict
- **[06-streaming/python/avro_example/ride_record.py](./06-streaming/python/avro_example/ride_record.py_docs.md)**: from typing import List, Dict
- **[06-streaming/python/avro_example/consumer.py](./06-streaming/python/avro_example/consumer.py_docs.md)**: from typing import Dict, List
- **[06-streaming/python/avro_example/ride_record_key.py](./06-streaming/python/avro_example/ride_record_key.py_docs.md)**: from typing import Dict
- **[06-streaming/python/streams-example/pyspark/producer.py](./06-streaming/python/streams-example/pyspark/producer.py_docs.md)**: from typing import Dict
- **[06-streaming/python/streams-example/pyspark/consumer.py](./06-streaming/python/streams-example/pyspark/consumer.py_docs.md)**: from typing import Dict, List
- *(... and 5 more occurrences)*

## unix

- **[05-batch/setup/pyspark.md](./05-batch/setup/pyspark.md_docs.md)**: Windows, you may have to do path conversion from unix-style to windows-style:

## unwrap

- **[05-batch/code/08_rdds.ipynb](./05-batch/code/08_rdds.ipynb_docs.md)**: "def unwrap(row):\n",

## upload_to_gcs

- **[cohorts/2022/week_2_data_ingestion/homework/solution.py](./cohorts/2022/week_2_data_ingestion/homework/solution.py_docs.md)**: def upload_to_gcs(bucket, object_name, local_file):
- **[cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py](./cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py_docs.md)**: # def upload_to_gcs(bucket, object_name, local_file):
- **[cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py](./cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py_docs.md)**: def upload_to_gcs(bucket, object_name, local_file):
- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: def upload_to_gcs(file_path, max_retries=3):
- **[03-data-warehouse/extras/web_to_gcs.py](./03-data-warehouse/extras/web_to_gcs.py_docs.md)**: def upload_to_gcs(bucket, object_name, local_file):

## url

- **[01-docker-terraform/2_docker_sql/data-loading-parquet.py](./01-docker-terraform/2_docker_sql/data-loading-parquet.py_docs.md)**: url = params.url

## urllib

- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: import urllib.request

## userdata

- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: from google.colab import userdata
- **[cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb](./cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb_docs.md)**: "from google.colab import userdata\n",

## validate_and_report

- **[repo_book_gen.py](./repo_book_gen.py_docs.md)**: def validate_and_report(self):

## various

- **[cohorts/2024/workshops/dlt_resources/workshop.ipynb](./cohorts/2024/workshops/dlt_resources/workshop.ipynb_docs.md)**: can add to your Python scripts to load data from various and often messy data sources into well-stru...
- **[cohorts/2025/workshops/dlt/data_ingestion_workshop.md](./cohorts/2025/workshops/dlt/data_ingestion_workshop.md_docs.md)**: âœ… **Extracting** data from various sources (APIs, databases, files).

## ver

- **[cohorts/2022/week_2_data_ingestion/airflow/docker-compose_2.3.4.yaml](./cohorts/2022/week_2_data_ingestion/airflow/docker-compose_2.3.4.yaml_docs.md)**: function ver() {
- **[cohorts/2022/week_2_data_ingestion/airflow/docker-compose.yaml](./cohorts/2022/week_2_data_ingestion/airflow/docker-compose.yaml_docs.md)**: function ver() {
- **[cohorts/2022/week_3_data_warehouse/airflow/docker-compose.yaml](./cohorts/2022/week_3_data_warehouse/airflow/docker-compose.yaml_docs.md)**: function ver() {

## verify_gcs_upload

- **[cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py](./cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py_docs.md)**: def verify_gcs_upload(blob_name):

## version

- **[cohorts/2023/week_6_stream_processing/homework.md](./cohorts/2023/week_6_stream_processing/homework.md_docs.md)**: eeper is removed from Kafka cluster starting from version 4.0 [x]

## videos

- **[cohorts/2023/week_2_workflow_orchestration/README.md](./cohorts/2023/week_2_workflow_orchestration/README.md_docs.md)**: Python code from videos is linked [below](#code-repository).

## web_to_gcs

- **[03-data-warehouse/extras/web_to_gcs.py](./03-data-warehouse/extras/web_to_gcs.py_docs.md)**: def web_to_gcs(year, service):

## webhooks

- **[cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md](./cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md_docs.md)**: - from webhooks to event queues

## which

- **[02-workflow-orchestration/flows/06_gcp_taxi_scheduled.yaml](./02-workflow-orchestration/flows/06_gcp_taxi_scheduled.yaml_docs.md)**: OPTIONS (description = 'The source filename from which the trip data was loaded.'),
- **[02-workflow-orchestration/flows/06_gcp_taxi.yaml](./02-workflow-orchestration/flows/06_gcp_taxi.yaml_docs.md)**: OPTIONS (description = 'The source filename from which the trip data was loaded.'),

## windowSerde

- **[06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java](./06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java_docs.md)**: var windowSerde = WindowedSerdes.timeWindowedSerdeFrom(String.cla

## windows

- **[05-batch/setup/windows.md](./05-batch/setup/windows.md_docs.md)**: ## Windows

## within

- **[06-streaming/pyflink/docker-compose.yml](./06-streaming/pyflink/docker-compose.yml_docs.md)**: way" #// Access services on the host machine from within the Docker container
- **[02-workflow-orchestration/README.md](./02-workflow-orchestration/README.md_docs.md)**: d` errors when connecting to the Postgres DB from within Kestra. This is because `host.docker.intern...

## would

- **[06-streaming/python/redpanda_example/consumer.py](./06-streaming/python/redpanda_example/consumer.py_docs.md)**: dded or the data types is changed, the Ride class would still work and produce-consume messages woul...

## yellow_taxi_data

- **[01-docker-terraform/2_docker_sql/upload-data.ipynb](./01-docker-terraform/2_docker_sql/upload-data.ipynb_docs.md)**: "CREATE TABLE yellow_taxi_data (\n",
- **[01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb](./01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb_docs.md)**: "CREATE TABLE yellow_taxi_data (\n",

## yellow_tripdata

- **[04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql](./04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql_docs.md)**: yellow_tripdata as (

## your

- **[04-analytics-engineering/dbt_cloud_setup.md](./04-analytics-engineering/dbt_cloud_setup.md_docs.md)**: * [Review your project settings](#review-your-project-settings)
- **[cohorts/2022/week_2_data_ingestion/airflow/1_setup_official.md](./cohorts/2022/week_2_data_ingestion/airflow/1_setup_official.md_docs.md)**: rename your gcp-service-accounts-credentials file to `google_
- **[cohorts/2022/week_3_data_warehouse/airflow/1_setup_official.md](./cohorts/2022/week_3_data_warehouse/airflow/1_setup_official.md_docs.md)**: rename your gcp-service-accounts-credentials file to `google_
- **[cohorts/2024/04-analytics-engineering/homework.md](./cohorts/2024/04-analytics-engineering/homework.md_docs.md)**: can do this locally using the ingested data from your Postgres database
- **[cohorts/2023/week_2_workflow_orchestration/README.md](./cohorts/2023/week_2_workflow_orchestration/README.md_docs.md)**: * Parametrizing the script from your flow
- **[cohorts/2023/week_2_workflow_orchestration/homework.md](./cohorts/2023/week_2_workflow_orchestration/homework.md_docs.md)**: a for Feb. 2019 and March 2019 loaded in GCS. Run your deployment to append this data to your BiqQue...
- **[cohorts/2023/week_4_analytics_engineering/homework.md](./cohorts/2023/week_4_analytics_engineering/homework.md_docs.md)**: can do this locally using the ingested data from your Postgres database

## zones

- **[01-docker-terraform/2_docker_sql/README.md](./01-docker-terraform/2_docker_sql/README.md_docs.md)**: to handle both csv and parquet files. (The lookup zones table which is needed later in this course i...


---
*Total unique keywords: 385*
