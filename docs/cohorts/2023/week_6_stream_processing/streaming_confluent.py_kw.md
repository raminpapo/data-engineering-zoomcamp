# Keywords: streaming_confluent.py

**File**: `cohorts/2023/week_6_stream_processing/streaming_confluent.py`

## Keyword Index

### CONFLUENT_CLOUD_CONFIG

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: from settings import CONFLUENT_CLOUD_CONFIG, GREEN_TAXI_TOPIC, FHV_TAXI_TOPIC, RIDES_TOPIC, A

### SparkSession

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: from pyspark.sql import SparkSession

### op_groupby

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: def op_groupby(df, column_names):

### parse_rides

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: def parse_rides(df, schema):

### pyspark

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: from pyspark.sql import SparkSession

### read_from_kafka

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: def read_from_kafka(consume_topic: str):

### settings

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: from settings import CONFLUENT_CLOUD_CONFIG, GREEN_TAXI_TOPIC,

### sink_console

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: def sink_console(df, output_mode: str = 'complete', processing_tim

### sink_kafka

- **Defined in**: [cohorts/2023/week_6_stream_processing/streaming_confluent.py](./streaming_confluent.py_docs.md)
- **Context**: def sink_kafka(df, topic, output_mode: str = 'complete'):


---
*Total keywords: 9*
