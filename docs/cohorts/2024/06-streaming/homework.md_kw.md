# Keywords: homework.md

**File**: `cohorts/2024/06-streaming/homework.md`

## Keyword Index

### KafkaProducer

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: from kafka import KafkaProducer

### Module

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: ## Module 6 Homework

### SparkSession

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: from pyspark.sql import SparkSession

### functions

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: from pyspark.sql import functions as F

### json

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: import json

### json_serializer

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: def json_serializer(data):

### kafka

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: Instead of Kafka, we will use Red Panda, which is a drop-in

### over

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: Iterate over the records in the dataframe

### peek

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: def peek(mini_batch, batch_id):

### pyspark

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: Module 5 Homework and learn about streaming with PySpark.

### time

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: import time

### types

- **Defined in**: [cohorts/2024/06-streaming/homework.md](./homework.md_docs.md)
- **Context**: from pyspark.sql import types


---
*Total keywords: 12*
